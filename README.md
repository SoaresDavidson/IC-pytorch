# Binariza√ß√£o da LeNet-5 com PyTorch

![Python](https://img.shields.io/badge/Python-3.9%2B-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)
![Status](https://img.shields.io/badge/Status-Em%20Desenvolvimento-yellow.svg)

Este reposit√≥rio cont√©m o c√≥digo-fonte e os resultados do projeto de Inicia√ß√£o Cient√≠fica focado na implementa√ß√£o e an√°lise de uma vers√£o binarizada da rede neural LeNet-5, utilizando o framework PyTorch e o dataset MNIST.

## üìñ Sum√°rio

* [Vis√£o Geral do Projeto](#-vis√£o-geral-do-projeto)
* [Conceitos Fundamentais](#-conceitos-fundamentais)
* [Estrutura do Reposit√≥rio](#-estrutura-do-reposit√≥rio)
* [Como Executar](#-como-executar)
* [Trabalhos Futuros](#-trabalhos-futuros)

---

## üî≠ Vis√£o Geral do Projeto

O objetivo deste trabalho √© explorar as Redes Neurais Binarizadas (BNNs) como uma t√©cnica de compress√£o e otimiza√ß√£o de modelos. Implementamos uma vers√£o binarizada da LeNet-5, uma arquitetura cl√°ssica para reconhecimento de d√≠gitos, e a treinamos no dataset MNIST. A an√°lise foca na compara√ß√£o entre o modelo binarizado e sua contraparte de precis√£o total em termos de acur√°cia, tamanho do modelo e velocidade de infer√™ncia (simulada).

---

## üß† Conceitos Fundamentais

#### LeNet-5
Uma das primeiras Redes Neurais Convolucionais (CNNs) de sucesso, proposta por Yann LeCun. Sua arquitetura √© um marco no campo da Vis√£o Computacional, sendo altamente eficaz para tarefas como o reconhecimento de d√≠gitos manuscritos.

#### Redes Neurais Binarizadas (BNNs)
BNNs s√£o redes neurais onde os pesos e/ou as ativa√ß√µes s√£o restringidos a apenas dois valores: **+1** ou **-1**. Essa restri√ß√£o dr√°stica oferece duas vantagens principais:
* **Redu√ß√£o de Mem√≥ria:** Cada peso pode ser armazenado com apenas 1 bit, resultando em uma compress√£o de at√© 32x em compara√ß√£o com modelos de 32 bits.
* **Efici√™ncia Computacional:** Multiplica√ß√µes de ponto flutuante, que s√£o caras, podem ser substitu√≠das por opera√ß√µes de bit `XNOR`, que s√£o extremamente r√°pidas em hardware especializado.

#### Straight-Through Estimator (STE)
A fun√ß√£o de binariza√ß√£o (`sign(x)`) possui gradiente zero em quase todos os pontos, o que impede o treinamento atrav√©s de backpropagation. O STE √© uma t√©cnica que contorna esse problema:
* **Na passagem `forward`:** Usa-se a fun√ß√£o `sign` normalmente.
* **Na passagem `backward`:** O gradiente √© "enganado" ou aproximado, geralmente usando a derivada de uma fun√ß√£o proxy como a identidade (`f(x) = x`) ou uma fun√ß√£o de clipping, permitindo que o erro flua atrav√©s da rede e os pesos sejam atualizados.

---

## üìÅ Estrutura do Reposit√≥rio

```
.
‚îú‚îÄ‚îÄ data/                  # Diret√≥rio para os datasets
‚îú‚îÄ‚îÄ main.py                # Script principal para treino e avalia√ß√£o do modelo
‚îú‚îÄ‚îÄ requirements.txt       # Lista de depend√™ncias do projeto
‚îî‚îÄ‚îÄ README.md              # Este arquivo
```

---

## üìù Trabalhos Futuros

- [x] Implementar a binariza√ß√£o das camadas convolucionais.
- [x] Implementar a binariza√ß√£o das camadas totalmente conectadas.
- [x] Binarizar as ativa√ß√µes, al√©m dos pesos.
- [ ] Experimentar diferentes fun√ß√µes de gradiente para o STE.
- [ ] Analisar o impacto da ordem das camadas (ex: `BN -> Ativa√ß√£o` vs. `Ativa√ß√£o -> BN`).
- [ ] Exportar o modelo para o formato ONNX para implanta√ß√£o em hardware de ponta.
